# 容器网络接口又名CNI

CNI常常与Kubernetes一起被提及。。它现在已成为Kubernetes不可或缺的一部分，默默地将它连接到不同节点上，并且能够很好地适应不同类型的网络解决方案（overlays, pure L3等）。话虽如此，它不仅存在于Kubernetes中。它本身就是一个独立的框架，它有自己的规范，不仅可以用于Kubernetes，还可以用于其他容器协调器，(如Mesos，Openshift等)一起使用。

CNI是什么？简而言之，它是网络命名空间和网络插件之间的接口。容器运行时（例如，Docker，Rocket）是网络命名空间。Network Plugin是遵循CNI规范的实现，用于获取容器运行时并将其配置(附加、分离)到插件实现的网络中。

该插件作为可执行文件存在。当调用时，它读入JSON配置，以获得配置容器所需的所有参数。该插件还可以调用其他插件来执行必要的任务，例如将IP分配给容器，这意味着CNI将IPAM与核心网络插件分开并保持正交，因此存在CNI-IPAM插件。

例如。在大多数附加过程中的情况下，JSON配置将包含“net”插件的条目，该插件执行创建将容器运行时连接到网络的接口的工作，以及一个告诉“ipam”的条目“net”插件用于IP分配的IPAM插件。

## CNI规格：

以下是通用参数：

![img](https://miro.medium.com/max/811/1*lPnJGWgIllaMz5l124KVUw.jpeg)

除了这些参数之外，配置还可以包含特定于正在使用的插件的字段。

![img](https://miro.medium.com/max/313/1*9XG1ZzLJg2owL67iasLpyA.jpeg)

> *旁注：*也可以为容器调用多个CNI插件，在这种情况下，需要在JSON配置中传递一组插件。在这种情况下，JSON规范需要一个名为“plugins”的字段，其中包含相应的插件顺序。

除了net和ipam插件之外，还有另一个名为“meta”插件的类。这些插件可以是核心网和ipam插件的包装器。元插件通常会将其CNI配置转换为核心网和ipam配置（例如，用于覆盖网络的flannel）或者将在核心插件的输出上进行某种额外配置（例如，端口映射，调整接口，sysctl，等等）

让我们进入设置。

所以，我有一个空闲的运行Ubuntu 16.04和已经建立了`cnitool`可执行文件`github.com/containernetworking/cni`和插件的可执行文件`github.com/containernetworking/plugins.`让我们创建一个名为*CNI*的目录在`$HOME`复制`cnitool`进去。现在让我们将所有插件可执行文件放在*cni*目录中的*plugins*目录中。此外，对于CNI配置，我们将在*cni*目录中创建一个名为*net.d*的目录。现在，它是空的。

```
vagrant @ machine-01：〜/ cni $  tree 。├──cnitool ├──net.d └──插件    ├──带宽    ├──桥    ├──DHCP     ├──绒布    ├──主机设备    ├──主机本地    ├──ipvlan     ├──回环    ├──macvlan     ├──端口映射    ├──PTP     ├──样品    ├──静态    ├──调谐    └──VLAN
```

现在，让我们来看看其中一个基本的CNI，即“bridge——桥接”插件。桥接插件将创建..井，桥（如果不存在）并通过veth对将容器运行时连接到它。就像众所周知的docker0接口一样。

假设我们已经启动了几个docker容器`--net=none`（没有连接到任何网络），让我们看一个示例CNI配置将它们添加到桥接网络。

![img](https://miro.medium.com/max/391/1*ZWDC2iLvUel5vinwLjN1Pw.jpeg)

30 mybridge.conf

这里的大多数项目都是可自我实现的。根据规定，网桥将充当容器的默认网关。这里使用的IPAM插件`host-local`是一个非常简单的IPAM，它从给定的范围中分配IP地址。（参见“ipam”部分中的“子网”字段）。让我们将这个json复制到*net.d*目录中。

现在让我们使用`cd`进入*cni*目录，看看我们将用于将容器添加到网络中的`cnitool`。

```
vagrant @ machine-01：〜/ cni $  ./cnitool -h cnitool：从网络命名空间添加或删除网络接口  cnitool add <net> <netns>   cnitool del <net> <netns>
```

它接收网络的名称（在*net.d*目录中的CNI配置文件中提到）和容器的网络命名空间的路径。

让我们运行`cnitool`now，并添加容器。

```
vagrant @ machine-01：〜/ cni $ sudo CNI_PATH = / home / vagrant / cni / plugins \ NETCONFPATH = / home / vagrant / cni / net.d \ ./cnitool add dbnet \ $（docker inspect mycnitest1 | \ jq。 [0] .NetworkSettings.SandboxKey | tr -d'“'）
```

`cnitool`将扫描*net.d*目录，挑选具有网络名称“dbnet”，然后添加容器“mycnitest1"的网络配置文件，我们还将对另一个容器重复相同的过程。

所以现在我们有两个容器连接到桥上。容器应该能够相互ping通就像漫游机器一样，还应该能够ping主机或google.com这样的外部网络。（*ipMasq = true*）。要从网络中删除容器，我们基本上会运行相同的命令，它非常小但却有非常重要的区别。（注意“add”和“del”的区别）

```
vagrant @ machine-01：〜/ cni $ sudo CNI_PATH = / home / vagrant / cni / plugins \ NETCONFPATH = / home / vagrant / cni / net.d \ ./cnitool del dbnet \ $（docker inspect mycnitest1 | \ jq。 [0] .NetworkSettings.SandboxKey | tr -d'“'）
```

我们可以在容器上调用更多CNI插件以进行额外配置。例如。我们可以使用该`portmap`插件将容器端口映射到主机。这是一个带链接的CNI配置示例。（一个接一个地运行一个CNI插件）。注意名为“plugins”和“portMappings = true”的新字段

![img](https://miro.medium.com/max/451/1*VlarxJU01cEv3GImmsSKIQ.jpeg)

30 mybridge-portmap.conflist

端口映射可以通过名为“CAP_ARGS”的环境变量作为运行时配置的一部分提供给“portmap”。让我们向网络添加一个容器，使用这个CNI配置并通过环境变量' CAP_ARGS '设置运行时参数。

```
vagrant @ machine-01：〜/ cni $ sudo CNI_PATH = / home / vagrant / cni / plugins \ NETCONFPATH = / home / vagrant / cni / net.d \ CAP_ARGS ='{“portMappings”：[{“hostPort”：6000 ，“containerPort”：3000，“protocol”：“tcp”}]}'\ ./cnitool add dbnet \ $（docker inspect mycnitest1 | \ jq。[0] .NetworkSettings.SandboxKey | tr -d'“'）
```

现在，我们应该能够通过连接到vagrant机器上的端口6000（*machine-01*）连接到端口3000上容器内运行的进程。

对感兴趣的人来说,如何实现这一点，您可以看看在调用这些插件时配置的iptables (NAT: ' iptables -L -t NAT ')规则。

OK！这是CNI的简短介绍，其中有一个非常基本的示例，只有一个VM（节点）。这应该为探索桥接器之外的其他核心网络插件(portmap插件等)提供了一个很好的基础。

当我们需要容器跨节点相互通信时，事情变得将非常有趣，允许非常有趣的网络拓扑（overlays, pure l3 routes)）。这就是flannel, calico, weave,等的元插件发挥作用的地方。我将会在另一篇文章中介绍这些内容。
